# Appendix. Data encryption and key management

## Network traffic

### In-transit encryption
Aito.ai uses SSL/TLS-encrypted traffic for all customer data. Aito does not provide
endpoints that would not be protected through SSL/TLS.

SSL/TLS is the default industry standard for client-server communication and
the major benefits are twofold:
1. It enforces the server identity by using a certificate authority. This means that
customers of the APIs are guarded against man-in-the-middle attacks, the most common
type of data theft and forgery attacks.
1. It manages a key-exchange and encryption protocol that makes sure that all the
traffic between client and host is encrypted, with a session based key, leaving it
immune to eavesdropping.

SSL/TLS is colloquially referred to 'https'-traffic. Aito certificates are generated
by the AWS, and use the best commonly available algorithms and protocols. Aito does
not use self-signed certificates, or certificates signed by untrusted entities.

### Certificate management
The reliability of the system is dependant on safe storage and management of the certificates
and must offer a way revoke compromised certificates. The Aito-service uses the AWS
Certificate Manager (CM), which offers all the required feature for such a system. The
certificates used by Aito are issued through the Certificate Manager. This guarantees
a level of safety according to the service level provided by the CM. Aito does not
manage or handle the certificates through other means, nor does Aito export the encryption
keys from the CM, as it might result compromise of the certificate security.

### API endpoint management
API endpoints are provided using AWS API Gateway. Each customer gets a separate subdomain
based endpoint, with a separate usage policy. This means that customers cannot
intentionally or un-intentionally cause DoS for the other users due to lack of
server resources.

Access restrictions to the endpoint based on e.g. IP or client certificate is not
possible at the moment.

## Index data
Aito stores user data both in transient and permanent (data-at-rest) fashion.

### Environment separation
Each customer environment is separated from other environments on multiple levels.
The data is stored in dedicated directories, and each respective environment manages
it's own data. Environments are also incapable of seeing or accessing the data of
other environments.

Each customer environment is in effect an own process, and is hence completely
separated from all other environments. The queries are parsed and resolved locally
in the environment, and no state is shared between environments.

### Permanent storage
The permanent storage is managed on disk volumes. Each customer environment operates
only on data stored through this environment and has no access to the data of
other customer environments.

### Transient data storage
Transient data storage in Aito is used for caching. The caches are specific
to the environment, and hence only the customer specific environment has access to
the cache. The same restrictions to access apply as with data at rest.

### System level access to stored data
Aito personnel with access to the running system, and thus the data at rest, is
limited to an absolute need-to-have basis. All maintenance operations to the
systems, as well as any operations to the live system is handled through managed
scripts and trusted path access, e.g. maintenance tools and scripts created for this
purpose. Tools and scripts are version controlled and treated in similar fashion
as all production code.

### Data backups
In order to prevent catastrophic loss of indexed data, Aito executes an hourly
backup of all the index data. The backups are performed for all customer
automatically. Backups
are retained until the subscription is terminated, after which all the backups are
removed on a best-effort basis. The removal is not automated, so this might occur
only at regular maintenance windows.


## Data access
Data access on Aito is authorised and restricted using a pair of customer specific
API-keys. The keys are separated into a read-only key, with access to the query
interfaces of the API, and a read-write key, which can be used to change the
schema or the data in the database.

API-keys are generated on environment setup. The generation is done by the AWS
API Gateway. The key is validated both on the API GW, as well as on the server level
when accessing the data, preventing attacks by other authorised users.

API keys cannot, as of now, be generated by the end-user. Changing the API key must
be done by Aito-administration. This feature is planned to be implemented as soon as
the Aito management UI is online.

API keys are not sent to the customer over public and unencrypted channels. Getting
hold of the API-key requires access to the AWS Console, and is thus protected as
described in [a separate appendix](./aws-operational.md).

### Internal authorisation and authentication
It is not possible to limit access or to compartmentalise the data inside the
Aito database. Aito is primarily meant for ML-operations over large datasets,
and thus optimised for efficiency over such data. Should there be need for
anonymising, hashing or limiting the access to parts of the data, this must be
handled outside of the Aito software, on the client side.

Compartmentalisation can be handled by using a separate Aito-instance for this purpose.
These two instances will share no state or data, and hence access can be managed
independently. These environments can, however, not be linked and hence it is not
possible to run queries over multiple environments in the Aito service.

### Query validation
All the Aito APIs accept JSON-encoded payloads. The JSON format is parsed using
pre-existing libraries and the messages are parsed according to the query schema
in strict fashion. This means that invalid queries are rejected and thus not applied
to the data.
